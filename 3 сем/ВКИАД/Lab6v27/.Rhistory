if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 1)
y1<-rnorm(20,mean = 10,sd = 1)
x2<-runif(10,-5,5)
y2<-runif(10,-5,5)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
View(xy)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
dat<-read.table("input.txt")
f<-as.data.frame(dat)
f
x<-as.numeric(c(dat[,2]))
n<-length(x)
n
averageLevel<-mean(x)
averageLevel
for( i in 1:n) if ( x[i] < averageLevel ) f[i,3] = 0 else f[i,3] = 1
f
lowbound<-(n+1-3*sqrt(n-1))/2
upperbound<-(n+1+3*sqrt(n-1))/2
lowbound
upperbound
f[1,4]=(5*f[1,2]+2*f[2,2]-f[3,2])/6
f[n,4]=(5*f[n,2]+2*f[n-1,2]-f[n-2,2])/6
for ( i in 3:n-1 ) f[i, 4]=(x[i-1]+x[i]+x[i+1])/3
f
f[1,5] <- -9
for ( j in 2:n ) f[j,5]=f[j-1,5]+1
for ( j in 1:n ) f[j,6]=f[j,5]*f[j,5]
for ( j in 1:n ) f[j,7]=f[j,2]*f[j,5]
f
a0<-sum(f[,2]) / n
a1<-sum(f[,7])/sum(f[,6])
a0
a1
for ( j in 1:n ) f[j,8]=f[j,5]*a1 + a0
f
for (i in 1:n) f[i,9]=i
x<-as.numeric(f[,9])
y<-as.numeric(f[,2])
z<-as.numeric(f[,4])
u<-as.numeric(f[,8])
#a. исходный ряд - green
plot(x, y, ylim=range(c(y,z, u)), type="l", col="green", lwd = 2)
par(new = TRUE)
#b. ряд, сглаженный методом трехуровневой скользящей средней - yellow
plot(x, z, ylim=range(c(y,z,u)), axes = FALSE, type="l", col="yellow", xlab = "", ylab = "", lwd = 2)
par(new = TRUE)
#c. ряд, сглаженный методом аналитического выравнивания - orange
plot(x, u, ylim=range(c(y,z,u)), axes = FALSE, type="l", col="orange", xlab = "", ylab = "", lwd = 2)
par(new = TRUE)
prognoz <- a1*10+ a0
prognoz
dat<-read.table("input.txt")
load("D:/2 курс/R/Lab4v27/.RData")
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
library(MASS)
x1<-rnorm(20,mean = 10,sd = 3.33)
y1<-rnorm(20,mean = 10,sd = 3.33)
x2<-runif(10,-2,4)
y2<-runif(10,-2,4)
xy<-cbind(c(x1, x2),c(y1, y2))
xy
cl<-kmeans(xy,2)
n<-30
n.train<-floor(n*0.7)#количество наблюдений для обучения
n.test<-n-n.train#количество наблюдений для тестирования
idx.train<-sample(1:n,n.train)
idx.test<-(1:n)[!(1:n %in% idx.train)]
data.train<-xy[idx.train,]
data.test<-xy[idx.test,]
cl.cluster<-cl$cluster
cl.train<-cl.cluster[idx.train]
cl.test<-cl.cluster[idx.test]
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
#оценим ошибку классификации
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green")) #70% - обучающ
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))#30% - тестов
points(xy[idw,],col="red", pch=3)#неверно классифицир в рез дискриминантн анализа
idd<-sample(1:n.train,n.train * 0.2)
for(i in idd) {
if(cl.train[i]==1) {cl.train[i]<-2}
else {cl.train[i]<-1};
}
model<-qda(data.train, cl.train)
cl.test_est<-predict(model, data.test)$class
sum(cl.test_est!=cl.test)/n.test
idw<-idx.test[cl.test_est!=cl.test]
idw
plot(xy, type="n")
legend("topleft",legend=c("1","2"),fill=c("blue","green"))
points(xy[idx.train,],pch=2, col=ifelse(cl.train==1,"blue","green"))
points(xy[idx.test,],pch=1, col=ifelse(cl.test==1,"blue","green"))
points(xy[idw,],col="red", pch=3)
points(xy[idx.train[idd],],pch=3)
